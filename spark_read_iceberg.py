from pyspark.sql import SparkSession

# =====================================================
# 1ï¸âƒ£ Táº¡o SparkSession vá»›i cáº¥u hÃ¬nh Iceberg + MinIO
# =====================================================
spark = (
    SparkSession.builder
    .appName("Read Iceberg from MinIO")
    # Cáº¥u hÃ¬nh catalog Ä‘á»ƒ Spark biáº¿t cÃ¡ch Ä‘á»c Iceberg table
    .config("spark.sql.catalog.my_catalog", "org.apache.iceberg.spark.SparkCatalog")
    .config("spark.sql.catalog.my_catalog.type", "hadoop")
    .config("spark.sql.catalog.my_catalog.warehouse", "s3a://warehouse/")  # Ä‘Æ°á»ng dáº«n gá»‘c
    # Cáº¥u hÃ¬nh MinIO (S3 endpoint)
    .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000")
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin")
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin")
    .config("spark.hadoop.fs.s3a.path.style.access", "true")
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem")
    .config("spark.sql.catalogImplementation", "hive")
    .getOrCreate()
)

# =====================================================
# 2ï¸âƒ£ Äá»c dá»¯ liá»‡u Parquet trá»±c tiáº¿p tá»« MinIO
# =====================================================

# ÄÆ°á»ng dáº«n dá»¯ liá»‡u tháº­t trong MinIO (bucket warehouse/MDI/mcc/data)
# Báº¡n cÃ³ thá»ƒ Ä‘iá»u chá»‰nh náº¿u cáº¥u trÃºc thá»±c táº¿ khÃ¡c.
data_path = "s3a://warehouse/MDI/mcc/data"

print("\n=== ğŸ” Äang Ä‘á»c dá»¯ liá»‡u tá»«:", data_path, "===\n")

# Äá»c cÃ¡c file parquet
df = spark.read.parquet(data_path)

# Hiá»ƒn thá»‹ schema vÃ  má»™t vÃ i dÃ²ng dá»¯ liá»‡u
print("=== âœ… Schema cá»§a dá»¯ liá»‡u ===")
df.printSchema()

print("=== âœ… Má»™t vÃ i dÃ²ng dá»¯ liá»‡u máº«u ===")
df.show(20, truncate=False)

# =====================================================
# 3ï¸âƒ£ (Tuá»³ chá»n) LÆ°u káº¿t quáº£ transform ra Iceberg table má»›i
# =====================================================
# VÃ­ dá»¥: lÆ°u láº¡i vÃ o warehouse/MDI/mcc/output_table
# df.writeTo("my_catalog.mdi.output_table").createOrReplace()

# Dá»«ng SparkSession
spark.stop()
